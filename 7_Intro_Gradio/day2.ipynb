{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
   "metadata": {},
   "source": [
    "# ¬°D√≠a de Gradio!\n",
    "\n",
    "Hoy crearemos interfaces de usuario utilizando el incre√≠blemente simple marco Gradio.\n",
    "\n",
    "## ¬°Prep√°rate para la alegr√≠a!\n",
    "\n",
    "Ten en cuenta que las pantallas de Gradio pueden aparecer en \"modo oscuro\" o \"modo claro\" seg√∫n la configuraci√≥n de tu computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1715421-cead-400b-99af-986388a97aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyDm\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuraci√≥n\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con√©ctese a OpenAI, Anthropic y Google; comente las l√≠neas de Claude o Google si no las est√° usando\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16e6021-6dc4-4397-985a-6679d6c8ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un mensaje gen√©rico del sistema: ¬°no m√°s IA adversarias sarc√°sticas!\n",
    "\n",
    "system_message = \"Eres un asistente √∫til\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ef9b69-ef31-427d-86d0-b8c799e1c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envolvamos una llamada a GPT-4o-mini en una funci√≥n simple\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef7d314-2b13-436b-b02d-8de3b72b193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La fecha de hoy es el 23 de octubre de 2023.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"¬øCual es la fecha de hoy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
   "metadata": {},
   "source": [
    "## Momento de crear nuestra interfaz de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc664b7a-c01d-4fea-a1de-ae22cdd5141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqu√≠ hay una funci√≥n simple\n",
    "\n",
    "def shout(text):\n",
    "    print(f\"Has llamado a la funci√≥n shout con el mensaje: {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083ea451-d3a0-4d13-b599-93ed49b975e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has llamado a la funci√≥n shout con el mensaje: Hola\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HOLA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f1f15a-122e-4502-b112-6ee2817dda32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has llamado a la funci√≥n shout con el mensaje: Hola\n"
     ]
    }
   ],
   "source": [
    "# La simplicidad de Gradio. Esto puede aparecer en \"modo claro\". M√°s adelante te mostrar√© c√≥mo hacerlo en modo oscuro.\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a359a4-685c-4c99-891c-bb4d1cb7f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://675ce1bed1ffda2f85.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://675ce1bed1ffda2f85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has llamado a la funci√≥n shout con el mensaje: hola\n"
     ]
    }
   ],
   "source": [
    "# Agregar share=True significa que se puede acceder a √©l de forma p√∫blica\n",
    "# Hay disponible un alojamiento m√°s permanente mediante una plataforma llamada Spaces de HuggingFace,\n",
    "# que abordaremos la pr√≥xima semana\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd87533a-ff3a-4188-8998-5bedd5ba2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "No config file found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has llamado a la funci√≥n shout con el mensaje: hola\n"
     ]
    }
   ],
   "source": [
    "# Agregar inbrowser=True abre una nueva ventana del navegador autom√°ticamente\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ec007-0314-48bf-84a4-a65943649215",
   "metadata": {},
   "source": [
    "## Forzar el modo oscuro\n",
    "\n",
    "Gradio aparece en modo claro o en modo oscuro seg√∫n la configuraci√≥n del navegador y la computadora. Hay una manera de forzar que Gradio aparezca en modo oscuro, pero Gradio recomienda no hacerlo ya que deber√≠a ser una preferencia del usuario (particularmente por razones de accesibilidad). Pero si desea forzar el modo oscuro para sus pantallas, a continuaci√≥n se muestra c√≥mo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8129afa-532b-4b15-b93c-aa9cca23a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define esta variable y luego pasa js=force_dark_mode al crear la interfaz\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc67b26-dd5f-406d-88f6-2306ee2950c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has llamado a la funci√≥n shout con el mensaje: hola\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entradas y Salidas\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[gr.Textbox(label=\"Tu Mensaje:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Respuesta:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f235288e-63a2-4341-935b-1441f9be969b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y ahora, cambiando la funci√≥n de \"shout\" a \"message_gpt\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Tu Mensaje:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Respuesta:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9a3262-e626-4e4b-80b0-aca152405e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usemos Markdown\n",
    "# ¬øTe preguntas por qu√© es importante configurar system_message cuando no se hace referencia a √©l en el c√≥digo que se encuentra debajo?\n",
    "# Estoy aprovechando que system_message es una variable global, que se usa en la funci√≥n message_gpt (√©chale un vistazo)\n",
    "# No es una gran pr√°ctica de ingenier√≠a de software, pero es bastante com√∫n durante la investigaci√≥n y el desarrollo de Jupyter Lab.\n",
    "\n",
    "system_message = \"Eres una asistente √∫til que responde en formato markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Tu mensaje:\")],\n",
    "    outputs=[gr.Markdown(label=\"Respuesta:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una llamada que transmita resultados\n",
    "# Si deseas un repaso de los generadores (la palabra clave \"yield\"),\n",
    "# Echa un vistazo al cuaderno de Python intermedio en la carpeta de la semana 1.\n",
    "\n",
    "\n",
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb1f789-ff11-4cba-ac67-11b815e29d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Tu :\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbc8e930-ba2a-4194-8f7c-044659150626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0066ffd-196e-4eaf-ad1e-d492958b62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_claude,\n",
    "    inputs=[gr.Textbox(label=\"Tu Mensaje:\")],\n",
    "    outputs=[gr.Markdown(label=\"Respuesta:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a70b9-2afe-4a7c-9bed-2429229e021b",
   "metadata": {},
   "source": [
    "## Una mejora menor\n",
    "\n",
    "He realizado una peque√±a mejora en este c√≥digo.\n",
    "\n",
    "Anteriormente, ten√≠a estas l√≠neas:\n",
    "\n",
    "```\n",
    "for chunk in result:\n",
    "yield chunk\n",
    "```\n",
    "\n",
    "En realidad, hay una forma m√°s elegante de lograr esto (que los expertos en Python podr√≠an llamar m√°s \"Pythonico\"):\n",
    "\n",
    "`yield from result`\n",
    "\n",
    "Abordo esto con m√°s detalle en el cuaderno de Python intermedio en la carpeta week1; √©chale un vistazo si quieres m√°s informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Modelo Desconocido\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Tu mensaje:\"), gr.Dropdown([\"GPT\", \"Claude\"], label=\"Selecciona un modelo:\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Respuesta:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933865b-654c-4b92-aa45-cf389f1eda3d",
   "metadata": {},
   "source": [
    "# C√≥mo crear un generador de folletos de empresa\n",
    "\n",
    "Ahora ya sabes c√≥mo hacerlo: ¬°es muy sencillo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7c49b-2e0e-45b3-92ce-93ca9f962ef4",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de leer las pr√≥ximas celdas</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Intente hacerlo usted mismo: vuelva al folleto de la empresa en la semana 1, d√≠a 5 y agregue una interfaz de usuario de Gradio al final. Luego, venga y observe la soluci√≥n.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1626eb2e-eee8-4183-bda5-1591b58ae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una clase para representar una p√°gina web\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No se ha encontrado t√≠tulo de la p√°gina\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"T√≠tulo de la Web:\\n{self.title}\\nContenido de la Web:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c701ec17-ecd5-4000-9f68-34634c8ed49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬°Muchas gracias a Bill G., que se dio cuenta de que una versi√≥n anterior de este programa ten√≠a un error! Ya est√° corregido.\n",
    "\n",
    "system_message = \"Eres un asistente que analiza el contenido de la p√°gina de inicio \\\n",
    "del sitio web de una empresa y crea un folleto breve sobre la empresa para posibles clientes, inversores y nuevos empleados.\\\n",
    "Responde en formato Markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5def90e0-4343-4f58-9d4a-0e36e445efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url, model):\n",
    "    prompt = f\"Genera un folleto de la empresa {company_name}. Esta es su p√°gina de destino:\\n\"    \n",
    "    prompt += Website(url).get_contents()\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Modelo Desconocido\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66399365-5d67-4984-9d47-93ed26c0bd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* Running on public URL: https://eeed84a31b697db468.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eeed84a31b697db468.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Nombre de la Empresa:\"),\n",
    "        gr.Textbox(label=\"Landing page, recuerda incluir http:// o https://\"),\n",
    "        gr.Dropdown([\"GPT\", \"Claude\"], label=\"Selecciona un modelo\")],\n",
    "    outputs=[gr.Markdown(label=\"Folleto:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ede97ca3-a0f8-4f6e-be17-d1de7fef9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-flash-latest')\n",
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac694d6-6deb-47e8-9421-d1d51a6d99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando Asistente Multi-Modelo...\n",
      "\n",
      "üìä Estado de las APIs:\n",
      "  OpenAI: ‚úÖ Configurada (empieza por sk-proj-)\n",
      "  Anthropic: ‚úÖ Configurada (empieza por sk-ant-)\n",
      "  Google: ‚úÖ Configurada (empieza por AIzaSyDm)\n",
      "\n",
      "üåê Lanzando interfaz de Gradio...\n",
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "No config file found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Asistente Multi-Modelo con Gradio\n",
    "Incluye GPT-4o-mini, Claude-3-Haiku y Gemini Pro\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar clientes de API\n",
    "openai_client = OpenAI()\n",
    "claude_client = anthropic.Anthropic()\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "# Mensaje del sistema por defecto\n",
    "DEFAULT_SYSTEM_MESSAGE = \"Eres un asistente √∫til que responde en formato markdown\"\n",
    "\n",
    "def verificar_apis():\n",
    "    \"\"\"Verificar que las APIs est√©n configuradas correctamente\"\"\"\n",
    "    apis_status = {}\n",
    "    \n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        apis_status['OpenAI'] = f\"‚úÖ Configurada (empieza por {openai_key[:8]})\"\n",
    "    else:\n",
    "        apis_status['OpenAI'] = \"‚ùå No configurada\"\n",
    "    \n",
    "    anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "    if anthropic_key:\n",
    "        apis_status['Anthropic'] = f\"‚úÖ Configurada (empieza por {anthropic_key[:7]})\"\n",
    "    else:\n",
    "        apis_status['Anthropic'] = \"‚ùå No configurada\"\n",
    "    \n",
    "    google_key = os.getenv('GOOGLE_API_KEY')\n",
    "    if google_key:\n",
    "        apis_status['Google'] = f\"‚úÖ Configurada (empieza por {google_key[:8]})\"\n",
    "    else:\n",
    "        apis_status['Google'] = \"‚ùå No configurada\"\n",
    "    \n",
    "    return apis_status\n",
    "\n",
    "def stream_gpt(prompt, system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "    \"\"\"Funci√≥n para streaming con GPT-4o-mini\"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        stream = openai_client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                result += chunk.choices[0].delta.content\n",
    "                yield result\n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå Error con GPT: {str(e)}\"\n",
    "\n",
    "def stream_claude(prompt, system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "    \"\"\"Funci√≥n para streaming con Claude-3-Haiku\"\"\"\n",
    "    try:\n",
    "        result = claude_client.messages.stream(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            system=system_message,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        response = \"\"\n",
    "        with result as stream:\n",
    "            for text in stream.text_stream:\n",
    "                if text:\n",
    "                    response += text\n",
    "                    yield response\n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå Error con Claude: {str(e)}\"\n",
    "\n",
    "def stream_gemini(prompt, system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "    \"\"\"Funci√≥n para streaming con Gemini Pro\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-flash-latest')\n",
    "        \n",
    "        # Combinar system message con el prompt del usuario\n",
    "        full_prompt = f\"{system_message}\\n\\nUsuario: {prompt}\"\n",
    "        \n",
    "        response = model.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=1000,\n",
    "            ),\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.text:\n",
    "                result += chunk.text\n",
    "                yield result\n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå Error con Gemini: {str(e)}\"\n",
    "\n",
    "def chat_with_model(prompt, model_choice, system_message):\n",
    "    \"\"\"Funci√≥n principal que maneja la selecci√≥n del modelo\"\"\"\n",
    "    if not prompt.strip():\n",
    "        yield \"Por favor, escribe un mensaje.\"\n",
    "        return\n",
    "    \n",
    "    # Usar el system message por defecto si est√° vac√≠o\n",
    "    if not system_message.strip():\n",
    "        system_message = DEFAULT_SYSTEM_MESSAGE\n",
    "    \n",
    "    # Seleccionar la funci√≥n seg√∫n el modelo elegido\n",
    "    if model_choice == \"GPT-4o-mini\":\n",
    "        yield from stream_gpt(prompt, system_message)\n",
    "    elif model_choice == \"Claude-3-Haiku\":\n",
    "        yield from stream_claude(prompt, system_message)\n",
    "    elif model_choice == 'gemini-flash-latest':\n",
    "        yield from stream_gemini(prompt, system_message)\n",
    "    else:\n",
    "        yield \"‚ùå Modelo no v√°lido seleccionado.\"\n",
    "\n",
    "def crear_interfaz():\n",
    "    \"\"\"Crear la interfaz de Gradio\"\"\"\n",
    "    \n",
    "    # CSS personalizado para mejorar la apariencia\n",
    "    css = \"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 800px !important;\n",
    "        margin: auto !important;\n",
    "    }\n",
    "    .model-info {\n",
    "        background-color: #f0f0f0;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(css=css, title=\"Asistente Multi-Modelo\") as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # ü§ñ Asistente Multi-Modelo\n",
    "        \n",
    "        Chatea con diferentes modelos de IA: **GPT-4o-mini**, **Claude-3-Haiku** y **Gemini Pro**\n",
    "        \n",
    "        Selecciona tu modelo preferido y comienza a conversar.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Mostrar estado de las APIs\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                apis_status = verificar_apis()\n",
    "                status_text = \"**Estado de las APIs:**\\n\\n\"\n",
    "                for api, status in apis_status.items():\n",
    "                    status_text += f\"- {api}: {status}\\n\"\n",
    "                gr.Markdown(status_text)\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                model_choice = gr.Dropdown(\n",
    "                    choices=[\"GPT-4o-mini\", \"Claude-3-Haiku\", 'gemini-flash-latest'],\n",
    "                    value=\"GPT-4o-mini\",\n",
    "                    label=\"üéØ Seleccionar Modelo\",\n",
    "                    info=\"Elige el modelo de IA que quieres usar\"\n",
    "                )\n",
    "                \n",
    "                system_message = gr.Textbox(\n",
    "                    label=\"‚öôÔ∏è Mensaje del Sistema (Opcional)\",\n",
    "                    placeholder=\"Eres un asistente √∫til que responde en formato markdown\",\n",
    "                    value=DEFAULT_SYSTEM_MESSAGE,\n",
    "                    lines=2,\n",
    "                    info=\"Define c√≥mo debe comportarse el asistente\"\n",
    "                )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt_input = gr.Textbox(\n",
    "                    label=\"üí¨ Tu Mensaje\",\n",
    "                    placeholder=\"Escribe tu pregunta o mensaje aqu√≠...\",\n",
    "                    lines=4\n",
    "                )\n",
    "                \n",
    "                submit_btn = gr.Button(\"üöÄ Enviar\", variant=\"primary\", size=\"lg\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                response_output = gr.Markdown(\n",
    "                    label=\"ü§ñ Respuesta del Asistente\",\n",
    "                    value=\"Selecciona un modelo y escribe tu mensaje para comenzar...\",\n",
    "                    height=400\n",
    "                )\n",
    "        \n",
    "        # Informaci√≥n sobre los modelos\n",
    "        with gr.Accordion(\"‚ÑπÔ∏è Informaci√≥n sobre los Modelos\", open=False):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### üîç Caracter√≠sticas de cada modelo:\n",
    "            \n",
    "            **GPT-4o-mini:**\n",
    "            - Desarrollado por OpenAI\n",
    "            - Excelente para tareas generales y programaci√≥n\n",
    "            - Respuestas r√°pidas y precisas\n",
    "            \n",
    "            **Claude-3-Haiku:**\n",
    "            - Desarrollado por Anthropic\n",
    "            - Enfocado en seguridad y utilidad\n",
    "            - Muy bueno para an√°lisis y razonamiento\n",
    "            \n",
    "            **Gemini Pro:**\n",
    "            - Desarrollado por Google\n",
    "            - Multimodal y vers√°til\n",
    "            - Excelente para tareas creativas y t√©cnicas\n",
    "            \"\"\")\n",
    "        \n",
    "        # Configurar eventos\n",
    "        submit_btn.click(\n",
    "            fn=chat_with_model,\n",
    "            inputs=[prompt_input, model_choice, system_message],\n",
    "            outputs=[response_output]\n",
    "        )\n",
    "        \n",
    "        # Tambi√©n permitir env√≠o con Enter\n",
    "        prompt_input.submit(\n",
    "            fn=chat_with_model,\n",
    "            inputs=[prompt_input, model_choice, system_message],\n",
    "            outputs=[response_output]\n",
    "        )\n",
    "        \n",
    "        # Ejemplos de prompts\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Expl√≠came qu√© es la inteligencia artificial en t√©rminos simples\"],\n",
    "                [\"Escribe un poema sobre la programaci√≥n\"],\n",
    "                [\"¬øCu√°les son las mejores pr√°cticas para aprender machine learning?\"],\n",
    "                [\"Ay√∫dame a crear una funci√≥n en Python para calcular n√∫meros primos\"],\n",
    "                [\"¬øCu√°l es la diferencia entre machine learning y deep learning?\"]\n",
    "            ],\n",
    "            inputs=[prompt_input],\n",
    "            label=\"üí° Ejemplos de Prompts\"\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal\"\"\"\n",
    "    print(\"üöÄ Iniciando Asistente Multi-Modelo...\")\n",
    "    \n",
    "    # Verificar APIs\n",
    "    apis_status = verificar_apis()\n",
    "    print(\"\\nüìä Estado de las APIs:\")\n",
    "    for api, status in apis_status.items():\n",
    "        print(f\"  {api}: {status}\")\n",
    "    \n",
    "    # Crear y lanzar la interfaz\n",
    "    demo = crear_interfaz()\n",
    "    \n",
    "    print(\"\\nüåê Lanzando interfaz de Gradio...\")\n",
    "    demo.launch(\n",
    "        share=False,  # Cambiar a True para crear enlace p√∫blico\n",
    "        inbrowser=True,  # Abrir autom√°ticamente en el navegador\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=None,  # Permitir que Gradio encuentre un puerto disponible autom√°ticamente\n",
    "        show_error=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
