{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998fbea8-d722-4588-b879-0dd75a401ae7",
   "metadata": {},
   "source": [
    "## Que vermeos\n",
    "- El impresionante ascenso de los Transformers.\n",
    "- Explicar personalizaciones de GPTs, Copilots y Agentes.\n",
    "- Comprender tokens, ventanas de contexto, parametros, costo de API.\n",
    "\n",
    "## El ascenso extraordinario del Transformer\n",
    "- 2017: Los cientificos de Google publican, el influyente articulo \"La atencion es todo loq ue necesitas\", que propone una nueva arquitectura de modelo llamada transformer.\n",
    "- 2018: Se lanza GPT-1, se usaban modelos de BERT un modelo de Google, por lo que no se usaba GPT.\n",
    "- 2019: GPT-2019.\n",
    "- 2020: GPT-3.\n",
    "- 2022: RLHF y ChatGPT, surge ChatGPT, y se uza para refinar el modelo aprendizaje por refuerzo por humanos.\n",
    "- 2023: GPT-4.\n",
    "- 2024: GPT-4o.\n",
    "- 2025: GPT-5.\n",
    "\n",
    "Las reacciones mundiales:\n",
    "- Primero, SORPRESA, El ChatGPT sorprende incluso a los expertos.\n",
    "- Entonces, escepticismo saludable. Texto predictivo super potente; el \"loro estocastico\".\n",
    "- Luego, la inteligencia emergente, **Las capacidades que surgen a gran escala**.\n",
    "## Breve nota del loro estocastico\n",
    "El \"loro estocástico\" es un término crítico acuñado para describir una limitación de los grandes modelos de lenguaje (LLMs) como yo. La expresión fue popularizada por Emily M. Bender y sus colegas en un artículo académico de 2021.\n",
    "¿Qué significa?\n",
    "La metáfora sugiere que estos modelos son como loros que repiten patrones del lenguaje que han \"escuchado\" en sus datos de entrenamiento, pero sin verdadera comprensión del significado. El término \"estocástico\" se refiere a que la generación de texto se basa en probabilidades estadísticas.\n",
    "La crítica principal:\n",
    "\n",
    "Los LLMs aprenden correlaciones entre palabras y patrones lingüísticos en enormes cantidades de texto\n",
    "No tienen experiencia directa del mundo real ni comprensión verdadera\n",
    "Pueden generar texto coherente y fluido sin \"entender\" realmente lo que dicen\n",
    "Pueden reproducir sesgos, información falsa o inconsistencias presentes en sus datos de entrenamiento\n",
    "\n",
    "El debate:\n",
    "Esta caracterización ha generado mucha discusión en el campo de la IA. Algunos argumentan que:\n",
    "\n",
    "Es una crítica válida sobre las limitaciones fundamentales de estos sistemas\n",
    "Otros sostienen que puede ser demasiado simplista y que estos modelos sí desarrollan ciertas formas de \"comprensión\" emergente\n",
    "\n",
    "## En el camino\n",
    "- Ingenieros de prompts. El ascenso, y caida? (Hablar con una maquina)\n",
    "- GPTs Personalizados y la tienda GPT. (Trabajar con la maquina)\n",
    "- Copilots: como MS copilot y Github Copilot. (Trabajar con la maquina)\n",
    "- Agentizacion: como Github Copilot Workspace.(Maquinas trabajando con maquinas, tienen memoria y acceso a aplicaciones, recursos, etc)\n",
    "\n",
    "## Numero de parametros de los modelos (escala logaritmica)\n",
    "En general los parametros y los pesos son sinonimos para nosotros. Los pesos son las palancas que se ajustan en el modelo, y controlan que tipo de resultados genera, cuando se le da una entrada, y partis de esta entrada da una salida o otra. En el caso del transformer, los pesos se modifican para que el modelo al darle una parabra, predizca una u otra palabra. Estos pesos se ajustan mas y mas hasta que el modelo mejora su capacidad de prediccion del siguiente token (palabra, no es lo mismo pero si su el origen del token). \n",
    "\n",
    "Modelos privados:\n",
    "- GPT-1 (2018): 117M.\n",
    "- GPT-2 : 1.5B.\n",
    "- GPT-3 : 175B\n",
    "- GPT-4 : 1.76T.\n",
    "- Ultimos modelos Fronterizos GPT-5(sin revelar).\n",
    "\n",
    "Modelos libres:\n",
    "- Gemma: 2B.\n",
    "- Llama 3.1: 405B.\n",
    "- Mixtral: 140B\n",
    "- GPT-OSS-120B: 120B.\n",
    "- GPT-OSS-20B: 20B.\n",
    "- Qwen3-VL: 250B.\n",
    "\n",
    "## Presentando los tokens\n",
    "Son la unidades individuales que se pasan a un modelo.\n",
    "\n",
    "- En los inicios, las redes neuronales se entrenaron a nivel de caracter, Predecir el siguiente caracter de la secuancia. Pequeno vocabulario, pero espra demasiado de la red.\n",
    "- Las redes neuronales se entrenaron con palabras, Predice la siguiente palabra de la secuancia. Mucho mas facil de aprender, pero con lleva enormes vocabularios con palabras raras omitidas.\n",
    "- El avance fue trabajar con trozos de palabras llamados 'tokens'(palabras a trozos), Un termino medio: vocabulario manejable e informacion util para la red neuronal, Ademas maneja con elegancia los fragmentos de palabras.\n",
    "\n",
    "Para poder poder observar como se tokenizan las palabras por parte de OpenAI. Podemos ir a la plataforma de \n",
    "```bash\n",
    "https://platform.openai.com/tokenizer\n",
    "```\n",
    "- Para palabras comunes, 1 palabra se asigna a 1 token, Observa como la separacion entre palabras forma parte del token.\n",
    "La palabras poco comunes ( E inventadas!) se divieden en varios tokens, En muchos casos, el significado aun se captura en los tokens: hand_crafted master_ers, Algunas veces, como en qu_ip, la palabra se rompe en varios fragmentos.\n",
    "- El como se genera el token, depende de como se escribe la frase y si son palabras conocidas o inventadas.\n",
    "- Observa como se tratan los numeros, lo que puede explicar porque los GPT anteriores tuvieron dificultades con las matematicas con mas de 3 digitos:\n",
    "\n",
    "Regla general: en la escritura tipica en ingles:\n",
    "- 1 token es aproximandamente 4 caracteres.\n",
    "- 1 token es aproximadamente 0l75 palabras.\n",
    "- Porlo tanto 1000 tokens es aproximadamente 750 palabras.\n",
    "  \n",
    "Las obras completas como Shakespeare contiene 900 000 palabras o 1.2 tokens.\n",
    "Obviamente, el recuento de tokens es mas alto para matematicas, terminos cientificos y codigo.\n",
    "\n",
    "![division tokens](image.png)\n",
    "![tokens vector](image_1.png)\n",
    "\n",
    "No existen reglas escritas de como generar tokesn, los diferentes modelos, tienen formas diferentes de generar diferentes tokens.\n",
    "\n",
    "## La ventana de Contexto\n",
    "La cantidad maxima de tokens que el modelo puede considerar al generar el siguiente token. El mensaje original, la conversacion posterior, el ultimo mensaje de entrada y casi todo el mensaje de salida se han rescrito de una manera diferente, respetando las reglas gramaticales. Se han aplicado el tono solicitado y se han traducido al spanish, manteniendo la longitud similar al texto original. Rige la capacidad del modelo para recordar referencias, contenido y contexto. Particularmente importante para prompts de multiples disparos que incluyen ejemplos o conversaciones largas. O quizas preguntas sobre las obras completas de Shakespare!\n",
    "\n",
    "En la practica la ventana de contexto, parece que chatgpt tiene una memoria, lo que realmente sucede es que el usario solicita las entradas, y las respuestas anteriores se vuelven a sumimistrar de nuevo, e gual que el mensaje dado, por lo que la ventana de contexto se va agotando, por loq ue la ventana de contexto es el total de las conversaciones hasta el mometo, las entradas, y la conversacion posterior hasta el proximo token que esta intentando producir, la ventana de contexto solo necesita ajustarse al mensaje actual, pero a medida que que avanzamos en la converzacion ,a ventana de contexto se va ajustando cada vez mas, porque necesita conocer lo que le has dicho anteriormente para mantener el contexto. Pero si quisieramos realizar por emplo realizar preguntas sobre todas la obras de Shakespeare, se necesita una una venta de contexto de al menos 1.2 millones de tokens, para preguntar algo.\n",
    "\n",
    "## Costes de la API\n",
    "Las interfaces de chat suelen tener un plan Pro, con una suscripcion mensual. Tiene limite de velocidad, pero sin cargo por uso.\n",
    "\n",
    "Las APIs generalmente no tienen suscripcion, pero cobram por cada llamada. El coste se basa en la cantidad de tokens de entrada y de salida.\n",
    "\n",
    "\n",
    "## Comparacion de costes de api, capacidades vs humano  y ventana de contexto.\n",
    "\n",
    "\n",
    "Podemos visitar \n",
    "```bash\n",
    "vellum.ia\n",
    "https://www.vellum.ai/llm-leaderboard?utm_source=google&utm_medium=organic\n",
    "```\n",
    "\n",
    "![ventanas de contexto](image_2.png)\n",
    "\n",
    "\n",
    "## Conclusiones clave\n",
    "- Ejecutar codigo para llamar a OpenIA y Ollama y resumir\n",
    "- Comparar los 6 principales LLM de frontier.\n",
    "- Analizar transformadores, tokens, ventanas de contextom costos de API y mas.\n",
    "El texto que se envia un LLM, esta tokenizado, todo lo que ve son token, por lo que no entienden bien las letras individuales, sino tokens, en modelos actuales dado los razonadores GPT5o lo logra hacer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
