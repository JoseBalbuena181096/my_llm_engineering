# ğŸ  Modelos Locales

Este mÃ³dulo explora la implementaciÃ³n y uso de Modelos de Lenguaje Grande (LLM) ejecutÃ¡ndose localmente, con especial Ã©nfasis en Ollama y alternativas a las APIs comerciales.

## ğŸ“‹ Contenido del MÃ³dulo

### ğŸ““ Notebooks y DocumentaciÃ³n
- `day2 EXERCISE.ipynb` - Ejercicios prÃ¡cticos con modelos locales
- `Notas.ipynb` - Conceptos teÃ³ricos y comparativas

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, serÃ¡s capaz de:

- âœ… Instalar y configurar Ollama para modelos locales
- âœ… Comparar ventajas y desventajas de modelos locales vs APIs
- âœ… Implementar soluciones usando modelos locales
- âœ… Optimizar el rendimiento de modelos locales
- âœ… Integrar modelos locales en aplicaciones Python

## ğŸ”§ ConfiguraciÃ³n de Ollama

### InstalaciÃ³n

#### Linux/Mac:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows:
Descargar desde [ollama.ai](https://ollama.ai/download)

### Modelos Disponibles

#### Modelos Recomendados:
```bash
# Modelo ligero y rÃ¡pido
ollama pull llama2

# Modelo mÃ¡s potente
ollama pull llama2:13b

# Modelo especializado en cÃ³digo
ollama pull codellama

# Modelo multimodal
ollama pull llava
```

### Iniciar Servidor:
```bash
ollama serve
```

## ğŸ†š Comparativa: Local vs API

### âœ… Ventajas de Modelos Locales

| Aspecto | Modelos Locales | APIs Comerciales |
|---------|----------------|------------------|
| **Privacidad** | âœ… Datos permanecen locales | âŒ Datos enviados a terceros |
| **Costo** | âœ… Sin costo por uso | âŒ Costo por token/request |
| **Latencia** | âœ… Baja latencia local | âŒ Depende de internet |
| **Disponibilidad** | âœ… Funciona offline | âŒ Requiere conexiÃ³n |
| **PersonalizaciÃ³n** | âœ… Modelos fine-tuneables | âŒ Limitado |

### âŒ Desventajas de Modelos Locales

| Aspecto | Modelos Locales | APIs Comerciales |
|---------|----------------|------------------|
| **Recursos** | âŒ Requiere hardware potente | âœ… Sin requisitos locales |
| **Calidad** | âŒ Menor que GPT-4 | âœ… Estado del arte |
| **Mantenimiento** | âŒ Actualizaciones manuales | âœ… AutomÃ¡tico |
| **Escalabilidad** | âŒ Limitada por hardware | âœ… Ilimitada |

## ğŸ› ï¸ ImplementaciÃ³n PrÃ¡ctica

### Ejemplo BÃ¡sico con Ollama:
```python
import ollama

# ConfiguraciÃ³n
MODEL = "llama2"

# Consulta simple
response = ollama.chat(
    model=MODEL,
    messages=[
        {"role": "user", "content": "Explica quÃ© es la robÃ³tica"}
    ]
)

print(response['message']['content'])
```

### IntegraciÃ³n con Aplicaciones:
```python
import ollama
import requests

class LocalLLM:
    def __init__(self, model="llama2"):
        self.model = model
        self.verify_model()
    
    def verify_model(self):
        """Verificar que el modelo estÃ© disponible"""
        try:
            models = ollama.list()
            available = [m.model for m in models.models]
            if self.model not in available:
                print(f"Modelo {self.model} no encontrado")
                return False
            return True
        except Exception as e:
            print(f"Error conectando con Ollama: {e}")
            return False
    
    def generate_response(self, prompt, context=""):
        """Generar respuesta usando el modelo local"""
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": context},
                    {"role": "user", "content": prompt}
                ]
            )
            return response['message']['content']
        except Exception as e:
            return f"Error: {e}"
```

## ğŸ” Casos de Uso Ideales

### âœ… CuÃ¡ndo Usar Modelos Locales:

1. **Datos Sensibles**: InformaciÃ³n confidencial o privada
2. **Aplicaciones Offline**: Sin conexiÃ³n a internet
3. **Costos Controlados**: Evitar costos por uso
4. **Baja Latencia**: Respuestas inmediatas
5. **ExperimentaciÃ³n**: Pruebas y desarrollo

### âŒ CuÃ¡ndo NO Usar Modelos Locales:

1. **Tareas Complejas**: Requieren mÃ¡xima calidad
2. **Recursos Limitados**: Hardware insuficiente
3. **Escalabilidad**: Muchos usuarios concurrentes
4. **Mantenimiento**: Sin recursos para actualizaciones

## ğŸ“Š Modelos Populares y Especificaciones

### Llama 2 Family:
- **llama2:7b** - 7B parÃ¡metros, ~4GB RAM
- **llama2:13b** - 13B parÃ¡metros, ~8GB RAM
- **llama2:70b** - 70B parÃ¡metros, ~40GB RAM

### Modelos Especializados:
- **codellama** - Optimizado para cÃ³digo
- **llava** - Multimodal (texto + imÃ¡genes)
- **mistral** - Eficiente y rÃ¡pido
- **neural-chat** - Conversacional

## ğŸš€ OptimizaciÃ³n de Rendimiento

### ConfiguraciÃ³n de Hardware:
```bash
# Verificar GPU disponible
nvidia-smi

# Configurar Ollama para GPU
export OLLAMA_GPU=1
```

### ParÃ¡metros de OptimizaciÃ³n:
```python
# ConfiguraciÃ³n para mejor rendimiento
response = ollama.chat(
    model="llama2",
    messages=[{"role": "user", "content": prompt}],
    options={
        "temperature": 0.7,      # Creatividad
        "top_p": 0.9,           # Diversidad
        "num_ctx": 2048,        # Contexto
        "num_predict": 512      # Longitud respuesta
    }
)
```

## ğŸ§ª Ejercicios PrÃ¡cticos

### Ejercicio 1: InstalaciÃ³n y ConfiguraciÃ³n
1. Instalar Ollama
2. Descargar modelo llama2
3. Probar consulta bÃ¡sica

### Ejercicio 2: Comparativa de Modelos
1. Probar diferentes modelos
2. Comparar calidad de respuestas
3. Medir tiempos de respuesta

### Ejercicio 3: IntegraciÃ³n en AplicaciÃ³n
1. Crear clase wrapper para Ollama
2. Implementar manejo de errores
3. Agregar logging y mÃ©tricas

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "Connection refused"
```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose
ollama serve

# Verificar puerto (default: 11434)
netstat -an | grep 11434
```

### Error: "Model not found"
```bash
# Listar modelos disponibles
ollama list

# Descargar modelo faltante
ollama pull nombre_modelo
```

### Error: "Out of memory"
```bash
# Usar modelo mÃ¡s pequeÃ±o
ollama pull llama2:7b

# Verificar RAM disponible
free -h
```

## ğŸ“ˆ PrÃ³ximos Pasos

DespuÃ©s de dominar modelos locales:

1. **MÃ³dulo 3**: Explorar modelos de frontera
2. **MÃ³dulo 4**: Profundizar en arquitectura Transformers
3. **MÃ³dulo 5**: Integrar en proyectos finales

## ğŸ’¡ Mejores PrÃ¡cticas

- ğŸ¯ **Elige el modelo adecuado**: Balancea calidad vs recursos
- ğŸ“Š **Monitorea recursos**: CPU, RAM, GPU usage
- ğŸ”„ **Actualiza regularmente**: Nuevos modelos y versiones
- ğŸ§ª **Experimenta**: Prueba diferentes configuraciones
- ğŸ“ **Documenta**: Registra configuraciones exitosas

---

**Â¡Domina los modelos locales y gana independencia tecnolÃ³gica!** ğŸ ğŸ¤–