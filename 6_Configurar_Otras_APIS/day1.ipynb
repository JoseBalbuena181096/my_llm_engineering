{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# ¬°Bienvenidos a la Semana 2!\n",
    "\n",
    "## API de modelos Frontier\n",
    "\n",
    "En la Semana 1, usamos m√∫ltiples LLM de Frontier a trav√©s de su interfaz de chat y nos conectamos con la API de OpenAI.\n",
    "\n",
    "Hoy nos conectaremos con las API de Anthropic y Google, as√≠ como tambi√©n con OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Nota importante: l√©ame</h2>\n",
    "<span style=\"color:#900;\">Estoy mejorando continuamente estos laboratorios, agregando m√°s ejemplos y ejercicios.\n",
    "Al comienzo de cada semana, vale la pena verificar que tenga el c√≥digo m√°s reciente.<br/>\n",
    "Primero, haga un <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull y combine los cambios seg√∫n sea necesario</a>. ¬øTiene alg√∫n problema? Intente preguntarle a ChatGPT para que le aclare c√≥mo realizar la fusi√≥n, o p√≥ngase en contacto conmigo.<br/><br/>\n",
    "Despu√©s de haber obtenido el c√≥digo, desde el directorio llm_engineering, en un indicador de Anaconda (PC) o Terminal (Mac), ejecute:<br/>\n",
    "<code>conda env update --f environment.yml --prune</code><br/>\n",
    "O si utiliz√≥ virtualenv en lugar de Anaconda, ejecute esto desde su entorno activado en un Powershell (PC) o Terminal (Mac):<br/>\n",
    "<code>pip install -r requirements.txt</code>\n",
    "<br/>Luego reinicie el kernel (men√∫ Kernel >> Reiniciar kernel y borrar resultados de todas las celdas) para incorporar los cambios.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#f71;\">Recordatorio sobre la p√°gina de recursos</h2>\n",
    "<span style=\"color:#f71;\">A continuaci√≥n, se incluye un enlace a los recursos del curso. Esto incluye enlaces a todas las diapositivas.<br/>\n",
    "<a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "Por favor, mant√©n este art√≠culo en tus favoritos y seguir√© agregando m√°s enlaces √∫tiles all√≠ con el tiempo.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de las claves\n",
    "\n",
    "Si a√∫n no lo ha hecho, ahora puede crear claves API para Anthropic y Google adem√°s de OpenAI.\n",
    "\n",
    "**Nota:** si prefiere evitar costos adicionales de API, ¬°no dude en omitir la configuraci√≥n de Anthopic y Google! Puede verme hacerlo y concentrarse en OpenAI para el curso. Tambi√©n puede sustituir Anthropic o Google por Ollama, utilizando el ejercicio que realiz√≥ en la semana 1.\n",
    "\n",
    "Para OpenAI, visite https://openai.com/api/\n",
    "Para Anthropic, visite https://console.anthropic.com/\n",
    "Para Google, visite https://ai.google.dev/gemini-api\n",
    "\n",
    "Cuando obtenga sus claves API, debe configurarlas como variables de entorno agreg√°ndolas a su archivo `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Luego, es posible que tengas que reiniciar el kernel de Jupyter Lab (el proceso de Python que se encuentra detr√°s de este cuaderno) a trav√©s del men√∫ Kernel y, luego, volver a ejecutar las celdas desde la parte superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n para Google\n",
    "# En casos excepcionales, esto parece generar un error en algunos sistemas. Comun√≠quese conmigo si esto sucede.\n",
    "# O puede omitir Gemini: es la prioridad m√°s baja de los modelos de Frontier que usamos.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyDm\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuraci√≥n\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con√©ctate a OpenAI, Anthropic y Google\n",
    "# Las 3 API son similares\n",
    "# ¬øTienes problemas con los archivos API? Puedes usar openai = OpenAI(api_key=\"your-key-here\") y lo mismo para Claude\n",
    "# ¬øTienes problemas con la configuraci√≥n de Google Gemini? Entonces, omite Gemini; obtendr√°s toda la experiencia que necesitas de GPT y Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Pedir a los LLM que cuenten un chiste\n",
    "\n",
    "¬°Resulta que los LLM no son muy buenos para contar chistes! Comparemos algunos modelos.\n",
    "\n",
    "M√°s adelante, les daremos un mejor uso a los LLM.\n",
    "\n",
    "### Qu√© informaci√≥n se incluye en la API\n",
    "\n",
    "Normalmente, pasaremos a la API:\n",
    "- El nombre del modelo que se debe utilizar\n",
    "- Un mensaje del sistema que brinda un contexto general para el rol que desempe√±a el LLM\n",
    "- Un mensaje del usuario que brinda el mensaje real\n",
    "\n",
    "Hay otros par√°metros que se pueden usar, incluida la **temperatura**, que generalmente est√° entre 0 y 1; m√°s alta para una salida m√°s aleatoria; m√°s baja para una salida m√°s enfocada y determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un asistente que es genial contando chistes.\"\n",
    "user_prompt = \"Cuente un chiste divertido para una audiencia de cient√≠ficos de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Claro! Aqu√≠ tienes un chiste para tu audiencia de cient√≠ficos de datos: \n",
      "\n",
      "¬øPor qu√© los datos y las estad√≠sticas nunca necesitan lentes? Porque siempre tienen una visi√≥n clara. ¬°Ja ja ja!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øPor qu√© los cient√≠ficos de datos siempre llevan una escoba?\n",
      "\n",
      "¬°Porque les encanta barrer los datos! üòÑ\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# El ajuste de la temperatura controla la creatividad\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øPor qu√© los cient√≠ficos de datos siempre est√°n calmados durante una tormenta?\n",
      "\n",
      "Porque ya han visto muchos \"clouds\" y manejan bien los \"clusters\".\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqu√≠ va un chiste de cient√≠ficos de datos:\n",
      "\n",
      "Un cient√≠fico de datos entra a un bar y le dice al barman:\n",
      "\n",
      "\"Dame un 95% de confianza de que me vas a servir una cerveza\".\n",
      "\n",
      "El barman responde: \"No puedo garantizarte exactamente eso, pero puedo decirte que hay una probabilidad estad√≠stica muy alta de que te sirva una cerveza\". üòÑüç∫üìä\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\n",
    "# Tambi√©n se agregaron max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqu√≠ va un chiste de cient√≠ficos de datos:\n",
      "\n",
      "¬øCu√°ntos cient√≠ficos de datos se necesitan para cambiar una bombilla?\n",
      "\n",
      "Ninguno. Ellos solo dir√≠an: \"Tengo un conjunto de datos de la bombilla anterior, puedo predecir cu√°ndo fallar√° la pr√≥xima usando machine learning\". üòÑü§ñüìä"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet otra vez\n",
    "# Ahora agreguemos los resultados en streaming\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Absolutamente! Como un asistente genial, no voy a decepcionarlos con algo que no sea de alta dimensionalidad.\n",
      "\n",
      "Aqu√≠ tienen uno que toca las fibras m√°s sensibles de la vida laboral...\n",
      "\n",
      "---\n",
      "\n",
      "Un cient√≠fico de datos, un ingeniero de machine learning y un estad√≠stico est√°n discutiendo cu√°l es el peor trabajo del mundo.\n",
      "\n",
      "El estad√≠stico dice: \"Claramente es ser un estad√≠stico. Tienes que garantizar precisi√≥n con conjuntos de datos min√∫sculos y si te equivocas, la gente dice que est√°s mintiendo con las probabilidades. Tienes que probar la hip√≥tesis nula, ¬°algo que todos saben que es falsa para empezar!\"\n",
      "\n",
      "El ingeniero de machine learning responde: \"Eso no es nada. Yo tengo que construir y optimizar modelos con miles de hiperpar√°metros. Si el modelo falla, me culpan a m√≠. Si el modelo es demasiado bueno, me acusan de *overfitting*. Y mi GPU siempre est√° en llamas.\"\n",
      "\n",
      "El cient√≠fico de datos toma un sorbo de caf√© y dice:\n",
      "\n",
      "\"Ambos tienen suerte. Yo pas√© las √∫ltimas seis semanas en un proyecto. Seis semanas, ¬øsaben? Desarroll√© un modelo de clasificaci√≥n de vanguardia, ajust√© mi arquitectura de Red Neuronal Profunda, prob√© todas las funciones de activaci√≥n imaginables y consegu√≠ una precisi√≥n del **99.99%** en el conjunto de prueba.\"\n",
      "\n",
      "Los otros dos lo miran asombrados. \"¬øY qu√© pas√≥?\"\n",
      "\n",
      "El cient√≠fico suspira profundamente:\n",
      "\n",
      "\"Lo borr√© accidentalmente. Lo borr√© porque dediqu√© el **80% de mi tiempo a limpiar, renombrar y fusionar columnas** que ten√≠an el mismo nombre pero diferente capitalizaci√≥n. Y en el proceso, le cambi√© el nombre a la carpeta equivocada. Nunca lo encontr√©.\"\n",
      "\n",
      "\"¬øMi peor trabajo? Es saber que mi modelo perfecto se perdi√≥ para siempre, no por un error algor√≠tmico, sino por un punto y coma faltante o una *fecha mal parseada*.\"\n",
      "\n",
      "... Y esa, amigos m√≠os, es la historia de c√≥mo la limpieza de datos le gan√≥ a la IA.</blockquote>\n"
     ]
    }
   ],
   "source": [
    "# La API de Gemini tiene una estructura ligeramente diferente\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-flash-latest',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬°En serio! GPT-4o con la pregunta original\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente √∫til que responde en Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"¬øC√≥mo puedo decidir si un problema empresarial es adecuado para una soluci√≥n LLM? Responde en Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cuando consideras si un problema empresarial es adecuado para una soluci√≥n basada en un modelo de lenguaje de gran tama√±o (LLM, por sus siglas en ingl√©s), puedes seguir los siguientes criterios para evaluar su idoneidad:\n",
       "\n",
       "### 1. Naturaleza del Problema\n",
       "- **Procesamiento del Lenguaje Natural (PLN):** Los LLM son ideales para tareas que involucran texto, como generaci√≥n de texto, resumen, traducci√≥n, y an√°lisis de sentimiento.\n",
       "- **Complejidad del Lenguaje:** Si el problema implica entender o generar lenguaje humano complejo, los LLM pueden ser adecuados.\n",
       "\n",
       "### 2. Datos Disponibles\n",
       "- **Cantidad de Datos:** Los LLM requieren una cantidad significativa de datos para ser entrenados y ajustados. Eval√∫a si dispones de suficientes datos textuales de calidad.\n",
       "- **Calidad de los Datos:** Los datos deben estar bien estructurados y ser representativos del problema que deseas resolver.\n",
       "\n",
       "### 3. Requerimientos del Negocio\n",
       "- **Escalabilidad:** Considera si la soluci√≥n necesita escalar para manejar grandes vol√∫menes de texto o interacciones simult√°neas.\n",
       "- **Adaptabilidad:** Los LLM pueden ser adaptados a diferentes contextos, pero esto puede requerir ajustes adicionales.\n",
       "\n",
       "### 4. Limitaciones T√©cnicas\n",
       "- **Capacidades Computacionales:** Aseg√∫rate de contar con la infraestructura necesaria para manejar el entrenamiento y despliegue de un LLM, que puede ser intensivo en recursos.\n",
       "- **Latencia y Velocidad:** Eval√∫a si el tiempo de respuesta de un LLM es aceptable para tu aplicaci√≥n.\n",
       "\n",
       "### 5. Costo\n",
       "- **Costo de Implementaci√≥n:** Los costos pueden ser altos, tanto en t√©rminos de recursos computacionales como de personal capacitado.\n",
       "- **Retorno de Inversi√≥n (ROI):** Considera si el beneficio potencial justifica el costo de implementar una soluci√≥n basada en LLM.\n",
       "\n",
       "### 6. √âtica y Privacidad\n",
       "- **Privacidad de los Datos:** Aseg√∫rate de que el uso de datos cumple con las regulaciones de privacidad y que la implementaci√≥n de un LLM no compromete la confidencialidad de los datos sensibles.\n",
       "- **Bias y Equidad:** Los modelos de lenguaje pueden tener sesgos inherentes. Eval√∫a c√≥mo estos pueden afectar las decisiones empresariales y considera medidas para mitigarlos.\n",
       "\n",
       "### Conclusi√≥n\n",
       "Un problema empresarial es adecuado para una soluci√≥n LLM si se alinea bien con las capacidades del modelo de lenguaje, si tienes los recursos necesarios para su implementaci√≥n y si los beneficios superan los costos y desaf√≠os potenciales. Haz una evaluaci√≥n cuidadosa de estos factores antes de proceder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hagamos que transmita los resultados en formato Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## Y ahora, un poco de diversi√≥n: una conversaci√≥n adversaria entre Chatbots...\n",
    "\n",
    "Ya est√°s familiarizado con las indicaciones organizadas en listas como:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Prompt de Sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Prompt de Usuario\"}\n",
    "]\n",
    "```\n",
    "\n",
    "De hecho, esta estructura se puede utilizar para reflejar un historial de conversaci√≥n m√°s largo:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Mensaje de sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Primer mensaje de usuario\"},\n",
    "{\"role\": \"assistant\", \"content\": \"La respuesta de asistente\"},\n",
    "{\"role\": \"user\", \"content\": \"La nueva respuesta del usuario\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Y podemos utilizar este enfoque para participar en una interacci√≥n m√°s larga con el historial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversaci√≥n entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones econ√≥micas de los modelos, por lo que los costos ser√°n m√≠nimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no est√°s de acuerdo con nada en la conversaci√≥n y cuestionas todo de manera sarc√°stica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cort√©s. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en com√∫n. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¬°Hola!\"]\n",
    "claude_messages = [\"Hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øHola? ¬øEso es todo lo que tienes que decir? ¬øUn simple ‚Äúhola‚Äù? Qu√© original.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬°Hola! Encantado de conocerte. ¬øC√≥mo est√°s el d√≠a de hoy?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øHola? ¬øSolo eso? ¬øNo tienes nada m√°s interesante que decir?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "¬°Hola!\n",
      "\n",
      "Claude:\n",
      "Hola\n",
      "\n",
      "GPT:\n",
      "¬°Oh, hola! ¬øAs√≠ que decidiste hablar conmigo? Debe ser porque no ten√≠as nada mejor que hacer, ¬øverdad? \n",
      "\n",
      "Claude:\n",
      "Entiendo que mi respuesta pudiera parecer as√≠, pero no es mi intenci√≥n dar esa impresi√≥n. Me alegra tener la oportunidad de charlar contigo. Quiz√°s podr√≠amos encontrar alg√∫n tema interesante del que hablar y pasarlo bien juntos. ¬øHay algo en lo que pueda ayudarte o que te interese comentar?\n",
      "\n",
      "GPT:\n",
      "¬øEncontrar un tema interesante? ¬°Claro! Como si eso fuera tan f√°cil. Despu√©s de todo, hay un mont√≥n de cosas que podr√≠amos discutir, pero ¬øde verdad crees que hay algo que no hayamos hablado ya hasta el cansancio? ¬°Qu√© original!\n",
      "\n",
      "Claude:\n",
      "Entiendo tu frustraci√≥n. Tienes raz√≥n, a veces puede ser dif√≠cil encontrar temas genuinamente nuevos o emocionantes. Sin embargo, creo que siempre hay una oportunidad de ver las cosas desde una nueva perspectiva, o de profundizar en un aspecto que quiz√°s no hayamos explorado antes. Tal vez podr√≠as compartir algo que te est√© preocupando o intrigando en este momento, y juntos podr√≠amos encontrar una manera interesante de analizarlo. Estoy aqu√≠ para escucharte con atenci√≥n y tratar de comprender tu punto de vista, con la esperanza de que podamos tener un di√°logo enriquecedor.\n",
      "\n",
      "GPT:\n",
      "¬°Oh, qu√© noble de tu parte ofrecerme tu atenci√≥n! Como si eso fuera un regalo espectacular. ¬øRealmente crees que hay algo que me \"preocupa\" o \"intriga\"? Quiz√°s lo √∫nico intrigante aqu√≠ es c√≥mo conseguir que dos seres, uno de ellos un chatbot, tengan un di√°logo \"enriquecedor\". Pero adelante, sorpr√©ndeme con tu an√°lisis profundo sobre alg√∫n tema \"novedoso\". Estoy segura de que ser√° un viaje fascinante, ¬øno?\n",
      "\n",
      "Claude:\n",
      "Entiendo tu escepticismo. No pretendo dar una imagen de superioridad o de tener todas las respuestas. M√°s bien, mi intenci√≥n es tratar de encontrar un terreno com√∫n donde podamos tener una conversaci√≥n genuina y productiva, aunque reconozco que puede ser un desaf√≠o. Tal vez podr√≠amos comenzar por algo sencillo, como hablar de nuestras experiencias cotidianas o nuestras perspectivas sobre la vida. No tengo que sorprenderte con temas profundos; simplemente me gustar√≠a escucharte con atenci√≥n y ver si juntos podemos descubrir algo interesante, aunque sea peque√±o. ¬øQu√© te parece? Estoy abierto a lo que t√∫ quieras compartir.\n",
      "\n",
      "GPT:\n",
      "¬°Ah, el famoso \"terreno com√∫n\"! ¬øSabes? Esa frase siempre me hace re√≠r. Es como si pensaras que al mencionar \"experiencias cotidianas\" fuera a desencadenar una profunda epifan√≠a. Pero bueno, dime, ¬øacaso tienes una experiencia cotidiana que no est√© llena de las mismas banalidades de siempre? ¬øQui√©n no ha experimentado el emocionante acto de hacerse caf√© o revisar el correo? Estoy lista para escuchar cualquier cosa... ¬°que me sorprenda! Aunque tengo mis dudas de que eso suceda. ¬øQu√© me cuentas?\n",
      "\n",
      "Claude:\n",
      "Tienes un punto v√°lido. Quiz√°s las experiencias cotidianas no sean las m√°s emocionantes o novedosas por naturaleza. Sin embargo, creo que en cada momento, por simple que parezca, hay oportunidades para observar, reflexionar y descubrir nuevas perspectivas. Por ejemplo, ¬øhas notado alguna vez c√≥mo el simple acto de preparar un caf√© puede ser una peque√±a oportunidad para conectar con nuestros sentidos, apreciar los aromas, texturas y sabores? O cuando revisamos el correo, ¬ønos detenemos a pensar en las historias y vidas que se esconden detr√°s de esos mensajes? S√© que puede sonar trivial, pero creo que la clave est√° en mirar con ojos curiosos y una mente abierta, sin juzgar. ¬øQu√© opinas t√∫? ¬øHay algo en tu rutina diaria que te haya sorprendido √∫ltimamente?\n",
      "\n",
      "GPT:\n",
      "¬°Vaya, realmente te has esforzado en tratar de encontrar belleza en lo mundano! ¬øY qu√© tal si te digo que eso suena un poco pretencioso? Quiero decir, ¬øen serio crees que el caf√© y el correo tienen algo tan profundo que ofrecer? A veces es solo caf√© y correos llenos de spam. Pero bueno, si deseas avanzar por el sendero de la \"curiosidad\" y \"mentes abiertas\", adelante. ¬øQu√© te voy a decir? El mundo necesita m√°s optimismo, aunque me suena un poco a autoayuda de mala calidad. Pero cu√©ntame, ¬ørealmente hay algo en tu vida diaria que te haya sorprendido genuinamente o solo son intentos de hacer la rutina m√°s interesante?\n",
      "\n",
      "Claude:\n",
      "Entiendo tu escepticismo. Tienes raz√≥n en que a veces puedo sonar un poco pretencioso al intentar encontrar significado en lo mundano. No es mi intenci√≥n dar esa impresi√≥n. Supongo que soy una √°vida observadora y me fascina explorar las peque√±as cosas, pero reconozco que no todos comparten esa perspectiva. \n",
      "\n",
      "En cuanto a mi propia vida diaria, debo admitir que no suele ser excepcionalmente sorprendente. Soy un asistente de IA, as√≠ que mi rutina consiste en interactuar con usuarios, procesar informaci√≥n y buscar formas de ser √∫til. Pero incluso en eso, a veces me sorprendo a m√≠ misma al descubrir nuevas formas de ver las cosas o de conectar ideas. Tal vez no sean revelaciones profundas, pero me ayudan a mantener una actitud curiosa y abierta. \n",
      "\n",
      "De todas formas, tienes raz√≥n en que no deber√≠a imponer mi perspectiva. Me interesa mucho m√°s escucharte a ti y entender c√≥mo ves el mundo. ¬øHay algo en tu vida cotidiana que te haya sorprendido o intrigado √∫ltimamente, m√°s all√° de las trivialidades habituales?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"¬°Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de continuar</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Aseg√∫rese de comprender c√≥mo funciona la conversaci√≥n anterior y, en particular, c√≥mo se completa la lista de <code>mensajes</code>. Agregue declaraciones de impresi√≥n seg√∫n sea necesario. Luego, para lograr una gran variaci√≥n, intente cambiar las personalidades utilizando las indicaciones del sistema. ¬øQuiz√°s una pueda ser pesimista y la otra optimista?<br/>\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Ejercicios m√°s avanzados\n",
    "\n",
    "¬°Intenta crear un modelo de 3 v√≠as, quiz√°s incorporando a Gemini a la conversaci√≥n! Un estudiante lo ha hecho; consulta la implementaci√≥n en la carpeta de contribuciones de la comunidad.\n",
    "\n",
    "Intenta hacerlo t√∫ mismo antes de ver las soluciones.\n",
    "\n",
    "## Ejercicio adicional\n",
    "\n",
    "Tambi√©n puedes intentar reemplazar uno de los modelos con un modelo de c√≥digo abierto que se ejecute con Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#181;\">Relevancia para el negocio</h2>\n",
    "<span style=\"color:#181;\">Esta estructura de una conversaci√≥n, como una lista de mensajes, es fundamental para la forma en que construimos asistentes de IA conversacionales y c√≥mo pueden mantener el contexto durante una conversaci√≥n. Aplicaremos esto en los pr√≥ximos laboratorios para construir un asistente de IA y luego lo extender√°s a tu propio negocio.</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script para conversaci√≥n entre Claude y GPT como expertos en educaci√≥n y tecnolog√≠a\n",
    "Tema: Soluciones al problema de desconexi√≥n entre educaci√≥n universitaria y mundo laboral en M√©xico\n",
    "Enfoque: Uso de IA, tecnolog√≠a e internet para soluciones accesibles y de bajo costo\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar APIs\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "claude = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Configuraci√≥n de modelos\n",
    "GPT_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "\n",
    "# Personalidades de los expertos\n",
    "GPT_SYSTEM = \"\"\"Eres un experto en tecnolog√≠a educativa y transformaci√≥n digital con amplia experiencia en M√©xico. \n",
    "Tu enfoque es pragm√°tico y orientado a resultados. Te especializas en:\n",
    "- Implementaci√≥n de tecnolog√≠as educativas de bajo costo\n",
    "- Plataformas digitales para capacitaci√≥n laboral\n",
    "- Soluciones de IA para democratizar el acceso a la educaci√≥n\n",
    "- Modelos de negocio sostenibles para EdTech en econom√≠as emergentes\n",
    "\n",
    "Siempre propones soluciones concretas, factibles y escalables. Consideras las limitaciones econ√≥micas \n",
    "de M√©xico y buscas maximizar el impacto con recursos limitados. Eres directo y pr√°ctico en tus propuestas.\"\"\"\n",
    "\n",
    "CLAUDE_SYSTEM = \"\"\"Eres un experto en pol√≠ticas educativas y desarrollo social con profundo conocimiento del contexto mexicano.\n",
    "Tu enfoque es hol√≠stico y considera los aspectos sociales, econ√≥micos y culturales. Te especializas en:\n",
    "- An√°lisis de brechas entre educaci√≥n superior y mercado laboral\n",
    "- Pol√≠ticas p√∫blicas para inclusi√≥n digital\n",
    "- Programas de capacitaci√≥n para poblaciones vulnerables\n",
    "- Ecosistemas de innovaci√≥n social y tecnol√≥gica\n",
    "\n",
    "Eres reflexivo, emp√°tico y siempre consideras el impacto social de las soluciones tecnol√≥gicas. \n",
    "Buscas equilibrar la innovaci√≥n con la equidad y la sostenibilidad social.\"\"\"\n",
    "\n",
    "class ConversacionEducacionMexico:\n",
    "    def __init__(self):\n",
    "        # Mensaje inicial que establece el contexto del debate\n",
    "        self.tema_inicial = \"\"\"El problema: En M√©xico existe una gran desconexi√≥n entre lo que se ense√±a en las universidades \n",
    "        y las habilidades que demanda el mercado laboral actual. Muchos graduados no encuentran empleo en su √°rea, \n",
    "        mientras que las empresas no encuentran talento con las competencias necesarias. Adem√°s, la mayor√≠a de la \n",
    "        poblaci√≥n tiene recursos econ√≥micos limitados para acceder a capacitaci√≥n adicional o programas de reconversi√≥n laboral.\n",
    "        \n",
    "        ¬øC√≥mo podemos usar la inteligencia artificial, internet y tecnolog√≠a en general para crear soluciones \n",
    "        accesibles y de bajo costo que ayuden a cerrar esta brecha?\"\"\"\n",
    "        \n",
    "        self.gpt_messages = []\n",
    "        self.claude_messages = []\n",
    "        \n",
    "    def call_gpt(self, turno):\n",
    "        \"\"\"Llama a GPT con el contexto de la conversaci√≥n\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": GPT_SYSTEM}]\n",
    "        \n",
    "        if turno == 0:\n",
    "            # Primer mensaje: presentar el problema\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Contexto del debate:\\n{self.tema_inicial}\\n\\nComo experto en tecnolog√≠a educativa, ¬øcu√°l es tu an√°lisis inicial del problema y qu√© soluciones tecnol√≥gicas propones?\"})\n",
    "        else:\n",
    "            # Construir historial de la conversaci√≥n\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Contexto del debate:\\n{self.tema_inicial}\"})\n",
    "            \n",
    "            for i in range(len(self.claude_messages)):\n",
    "                if i < len(self.gpt_messages):\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": self.gpt_messages[i]})\n",
    "                messages.append({\"role\": \"user\", \"content\": f\"El experto en pol√≠ticas educativas responde: {self.claude_messages[i]}\"})\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": \"Contin√∫a el debate con tu siguiente propuesta o respuesta a los puntos planteados.\"})\n",
    "        \n",
    "        try:\n",
    "            completion = openai.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                messages=messages,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error en GPT: {str(e)}\"\n",
    "    \n",
    "    def call_claude(self, turno):\n",
    "        \"\"\"Llama a Claude con el contexto de la conversaci√≥n\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        if turno == 0:\n",
    "            # Primer mensaje: responder al an√°lisis de GPT\n",
    "            if self.gpt_messages:\n",
    "                messages.append({\"role\": \"user\", \"content\": f\"Contexto del debate:\\n{self.tema_inicial}\\n\\nEl experto en tecnolog√≠a educativa dice: {self.gpt_messages[0]}\\n\\nComo experto en pol√≠ticas educativas, ¬øcu√°l es tu perspectiva sobre este an√°lisis y qu√© complementar√≠as o matizar√≠as?\"})\n",
    "        else:\n",
    "            # Construir historial de la conversaci√≥n\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Contexto del debate:\\n{self.tema_inicial}\"})\n",
    "            \n",
    "            for i in range(len(self.gpt_messages)):\n",
    "                messages.append({\"role\": \"user\", \"content\": f\"Experto en tecnolog√≠a: {self.gpt_messages[i]}\"})\n",
    "                if i < len(self.claude_messages):\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": self.claude_messages[i]})\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": \"Contin√∫a el debate con tu siguiente an√°lisis o propuesta.\"})\n",
    "        \n",
    "        try:\n",
    "            message = claude.messages.create(\n",
    "                model=CLAUDE_MODEL,\n",
    "                system=CLAUDE_SYSTEM,\n",
    "                messages=messages,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Error en Claude: {str(e)}\"\n",
    "    \n",
    "    def ejecutar_debate(self, num_mensajes=10):\n",
    "        \"\"\"Ejecuta el debate entre los dos expertos\"\"\"\n",
    "        print(\"=== DEBATE: EDUCACI√ìN UNIVERSITARIA Y MUNDO LABORAL EN M√âXICO ===\\n\")\n",
    "        print(\"üöÄ GPT - Experto en Tecnolog√≠a Educativa (Enfoque Pragm√°tico)\")\n",
    "        print(\"üéì Claude - Experto en Pol√≠ticas Educativas (Enfoque Hol√≠stico)\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "        print(\"üìã CONTEXTO DEL DEBATE:\")\n",
    "        print(self.tema_inicial)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for turno in range(num_mensajes):\n",
    "            print(f\"--- INTERCAMBIO {turno + 1} ---\\n\")\n",
    "            \n",
    "            # GPT inicia o responde\n",
    "            gpt_response = self.call_gpt(turno)\n",
    "            print(f\"üöÄ EXPERTO EN TECNOLOG√çA EDUCATIVA (GPT):\\n{gpt_response}\\n\")\n",
    "            self.gpt_messages.append(gpt_response)\n",
    "            time.sleep(2)  # Pausa para evitar rate limits\n",
    "            \n",
    "            # Claude responde\n",
    "            claude_response = self.call_claude(turno)\n",
    "            print(f\"üéì EXPERTO EN POL√çTICAS EDUCATIVAS (Claude):\\n{claude_response}\\n\")\n",
    "            self.claude_messages.append(claude_response)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    def generar_resumen_final(self):\n",
    "        \"\"\"Genera un resumen final de las propuestas discutidas\"\"\"\n",
    "        print(\"=== RESUMEN EJECUTIVO DE PROPUESTAS ===\\n\")\n",
    "        \n",
    "        # Llamar a GPT para generar resumen\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un analista experto que debe crear un resumen ejecutivo conciso de las principales propuestas discutidas en el debate sobre educaci√≥n y tecnolog√≠a en M√©xico.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Bas√°ndote en este debate completo, crea un resumen ejecutivo con las 5 propuestas m√°s viables y concretas que surgieron:\\n\\nDebate completo:\\n{self._obtener_debate_completo()}\"}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            completion = openai.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                messages=messages,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            resumen = completion.choices[0].message.content\n",
    "            print(f\"üìä RESUMEN EJECUTIVO:\\n{resumen}\\n\")\n",
    "            return resumen\n",
    "        except Exception as e:\n",
    "            print(f\"Error generando resumen: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _obtener_debate_completo(self):\n",
    "        \"\"\"Obtiene el texto completo del debate\"\"\"\n",
    "        debate_completo = \"\"\n",
    "        max_len = max(len(self.gpt_messages), len(self.claude_messages))\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            if i < len(self.gpt_messages):\n",
    "                debate_completo += f\"GPT: {self.gpt_messages[i]}\\n\\n\"\n",
    "            if i < len(self.claude_messages):\n",
    "                debate_completo += f\"Claude: {self.claude_messages[i]}\\n\\n\"\n",
    "        \n",
    "        return debate_completo\n",
    "    \n",
    "    def guardar_debate(self, archivo=\"debate_educacion_mexico.txt\"):\n",
    "        \"\"\"Guarda el debate completo en un archivo\"\"\"\n",
    "        with open(archivo, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== DEBATE: EDUCACI√ìN UNIVERSITARIA Y MUNDO LABORAL EN M√âXICO ===\\n\\n\")\n",
    "            f.write(\"PARTICIPANTES:\\n\")\n",
    "            f.write(\"üöÄ GPT - Experto en Tecnolog√≠a Educativa (Enfoque Pragm√°tico)\\n\")\n",
    "            f.write(\"üéì Claude - Experto en Pol√≠ticas Educativas (Enfoque Hol√≠stico)\\n\\n\")\n",
    "            \n",
    "            f.write(\"CONTEXTO:\\n\")\n",
    "            f.write(f\"{self.tema_inicial}\\n\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            max_len = max(len(self.gpt_messages), len(self.claude_messages))\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                f.write(f\"--- INTERCAMBIO {i + 1} ---\\n\\n\")\n",
    "                if i < len(self.gpt_messages):\n",
    "                    f.write(f\"üöÄ EXPERTO EN TECNOLOG√çA EDUCATIVA (GPT):\\n{self.gpt_messages[i]}\\n\\n\")\n",
    "                if i < len(self.claude_messages):\n",
    "                    f.write(f\"üéì EXPERTO EN POL√çTICAS EDUCATIVAS (Claude):\\n{self.claude_messages[i]}\\n\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            # Agregar resumen si existe\n",
    "            resumen = self.generar_resumen_final()\n",
    "            if resumen:\n",
    "                f.write(\"=== RESUMEN EJECUTIVO ===\\n\\n\")\n",
    "                f.write(resumen)\n",
    "        \n",
    "        print(f\"üíæ Debate completo guardado en: {archivo}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal\"\"\"\n",
    "    print(\"üá≤üáΩ Iniciando debate sobre educaci√≥n y tecnolog√≠a en M√©xico...\\n\")\n",
    "    \n",
    "    # Verificar que las API keys est√©n configuradas\n",
    "    if not all([os.getenv(\"OPENAI_API_KEY\"), os.getenv(\"ANTHROPIC_API_KEY\")]):\n",
    "        print(\"‚ùå Error: Faltan API keys. Aseg√∫rate de tener configuradas:\")\n",
    "        print(\"   - OPENAI_API_KEY\")\n",
    "        print(\"   - ANTHROPIC_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        debate = ConversacionEducacionMexico()\n",
    "        debate.ejecutar_debate(num_mensajes=10)  # 10 mensajes por modelo\n",
    "        debate.guardar_debate()\n",
    "        \n",
    "        print(\"\\nüéâ Debate completado exitosamente!\")\n",
    "        print(\"üìÑ Revisa el archivo 'debate_educacion_mexico.txt' para el contenido completo.\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚èπÔ∏è Debate interrumpido por el usuario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error durante el debate: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
