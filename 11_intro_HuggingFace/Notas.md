## ¿Qué haremos?
- Usar la plataforma de HuggingFace, que ofrece una amplia variedad de modelos de lenguaje.
- Utilizar los modelos, conjuntos de datos y espacios
- Utilizar Google Colab para programar en un entorno GPU de alto rendimiento.


### Plataforma de HuggingFace 
La plataforma omnipresente para ingenieros de LLM.

- Modelos:
Más de 800,000 modelos de código abierto de diversas formas y tamaños
- Datasets:
Un caudal de 200 000 conjuntos de datos.
- Espacios:
Aplicaciones, muchas contruidas en Gradio, incluyendo Clasificaciones.

- Bibliortecas de HuggingFace
y la asombrosa ventaja que obtenemos al utilizar estas bibliotecas.

- hub: 
Un repositorio centralizado para modelos, conjuntos de datos y espacios.
- datasets:
Biblioteca de HuggingFace para acceder y utilizar conjuntos de datos.
- transformers:
Biblioteca de HuggingFace para acceder y utilizar modelos de lenguaje.
- peft:
Biblioteca de HuggingFace para acceder y utilizar modelos de lenguaje finetuned (ajuste fino del llm).
- trl:
Biblioteca de HuggingFace para acceder y utilizar modelos de lenguaje finetuned (ajuste fino del llm) con Reinforcement Learning (Modelado por recompenza, political proximal optimization).

- accelerate: permite que los los tramsformers se ejecuten de forma distribuida, en diferente gpus.

## Google colab - Programar en un entorno GPU de alto rendimiento.
Google colab:
- Ejecutar un Jupyter notebook en un entorno GPU de alto rendimiento.
- Colaboracion con otros.
- Integra otros servicios de google.

Ejecuciónes:
- Basadas en GPU
- Tiempos de ejecución de GPU de menor especificicación para un menor coste.
- Tiempos de ejecución de ejecución de GPU de mayor especificicación para un mayor rendimiento, ejecuciones que consumen muchos recursos.




